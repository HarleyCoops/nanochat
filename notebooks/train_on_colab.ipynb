{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ac94335",
      "metadata": {},
      "source": [
        "# Nanochat Full Training Pipeline on Google Colab\n",
        "\n",
        "Run the complete nanochat \"speedrun\" pipeline end-to-end on Colab: environment bootstrapping, tokenizer training, pretraining, midtraining, supervised finetuning, optional RL, evaluation, and export."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cadd2f40",
      "metadata": {},
      "source": [
        "## Pipeline Overview\n",
        "- Prepare the runtime, GPU, and optional Google Drive storage.\n",
        "- Clone the repo and install everything with `uv`.\n",
        "- Build the Rust tokenizer, download datasets, and fetch evaluation bundles.\n",
        "- Train through base ? mid ? SFT stages (optional RL) with periodic evaluations.\n",
        "- Chat with the model and export checkpoints for downstream use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d385129",
      "metadata": {},
      "source": [
        "## Before You Start\n",
        "- In Colab select `Runtime ? Change runtime type ? GPU` before running cells.\n",
        "- Free-tier T4 works with the `t4_quick` preset (expect a few hours). Multi-GPU runs can switch to `speedrun_full`.\n",
        "- Long sessions should sync checkpoints to Drive to guard against disconnects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c026b49",
      "metadata": {},
      "source": [
        "## Step 0: Verify GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e74f79",
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a87305",
      "metadata": {},
      "source": [
        "## Step 1 (Optional): Mount Google Drive for checkpoints\n",
        "Skip this step if you prefer manual downloads or are not running inside Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4684cb17",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive')\n",
        "    checkpoint_dir = '/content/drive/MyDrive/nanochat_checkpoints'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(f'Drive mounted. Checkpoints will sync to {checkpoint_dir}')\n",
        "except ImportError:\n",
        "    print('google.colab not available; skipping Drive mount.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89de9c52",
      "metadata": {},
      "source": [
        "## Step 2: Clone (or update) the nanochat repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b701d89",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "cd /content\n",
        "if [ ! -d 'nanochat' ]; then\n",
        "  git clone https://github.com/HarleyCoops/nanochat.git\n",
        "else\n",
        "  cd nanochat\n",
        "  git pull --ff-only\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75ffcfa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/nanochat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa25983",
      "metadata": {},
      "source": [
        "## Step 3: Configure experiment presets and environment\n",
        "Choose the preset that matches your hardware. Update any values before running downstream cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5583f23a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = '/content/nanochat'\n",
        "os.environ['PROJECT_ROOT'] = PROJECT_ROOT\n",
        "\n",
        "CONFIG_PRESETS = {\n",
        "    't4_quick': {\n",
        "        'dataset_shards': 40,\n",
        "        'tokenizer_max_chars': 200_000_000,\n",
        "        'model_depth': 12,\n",
        "        'device_batch_size': 8,\n",
        "        'max_seq_len': 2048,\n",
        "        'num_iterations': 6000,\n",
        "        'nproc': 1,\n",
        "        'run_name': 'colab_t4',\n",
        "    },\n",
        "    'speedrun_full': {\n",
        "        'dataset_shards': 240,\n",
        "        'tokenizer_max_chars': 2_000_000_000,\n",
        "        'model_depth': 20,\n",
        "        'device_batch_size': 32,\n",
        "        'max_seq_len': 2048,\n",
        "        'num_iterations': 21400,\n",
        "        'nproc': 8,\n",
        "        'run_name': 'speedrun_d20',\n",
        "    },\n",
        "}\n",
        "\n",
        "ACTIVE_PRESET = 't4_quick'  # change to 'speedrun_full' when you have >=8 GPUs\n",
        "CONFIG = CONFIG_PRESETS[ACTIVE_PRESET].copy()\n",
        "CONFIG.update({\n",
        "    'dataset_cache_dir': str(Path.home() / '.cache' / 'nanochat'),\n",
        "    'eval_bundle_dir': str(Path.home() / '.cache' / 'nanochat' / 'eval_bundle'),\n",
        "    'eval_bundle_url': 'https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip',\n",
        "    'drive_checkpoints_dir': '/content/drive/MyDrive/nanochat_checkpoints',\n",
        "    'wandb_project': 'nanochat-colab',\n",
        "    'wandb_login': False,\n",
        "    'chat_prompt': 'Hello! Summarize what Nanochat is.',\n",
        "})\n",
        "\n",
        "extra_paths = ['/root/.local/bin', str(Path.home() / '.cargo' / 'bin')]\n",
        "for path in extra_paths:\n",
        "    if os.path.isdir(path) and path not in os.environ.get('PATH', ''):\n",
        "        os.environ['PATH'] = f\"{path}:{os.environ['PATH']}\"\n",
        "\n",
        "for key, value in CONFIG.items():\n",
        "    os.environ[f'NANOCHAT_{key.upper()}'] = str(value)\n",
        "\n",
        "print(f'Active preset: {ACTIVE_PRESET}')\n",
        "print(json.dumps(CONFIG, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a26e924",
      "metadata": {},
      "source": [
        "## Step 4: Install uv and project dependencies\n",
        "Mirrors the `speedrun.sh` bootstrap using uv-managed virtual environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "758bf746",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if ! command -v uv >/dev/null 2>&1; then\n",
        "  curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "  export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "if [ ! -d '.venv' ]; then\n",
        "  uv venv\n",
        "fi\n",
        "uv sync\n",
        "uv pip install maturin --upgrade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c26221",
      "metadata": {},
      "source": [
        "## Step 5: Install Rust and build the tokenizer extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ae9e83",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ ! -f \"$HOME/.cargo/env\" ]; then\n",
        "  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "fi\n",
        "source \"$HOME/.cargo/env\"\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run maturin develop --release --manifest-path rustbpe/Cargo.toml\n",
        "rustc --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb06669",
      "metadata": {},
      "source": [
        "## Step 6: Download pretraining shards\n",
        "Adjust `CONFIG['dataset_shards']` if storage or bandwidth is limited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa271caf",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_DATASET_SHARDS\n",
        "\n",
        "set -euo pipefail\n",
        "cd \"$PROJECT_ROOT\"\n",
        "echo \"Downloading ${NANOCHAT_DATASET_SHARDS} shards into ~/.cache/nanochat\"\n",
        "uv run python -m nanochat.dataset -n \"${NANOCHAT_DATASET_SHARDS}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762eabec",
      "metadata": {},
      "source": [
        "## Step 7: Train and evaluate the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921cc703",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_TOKENIZER_MAX_CHARS\n",
        "set -euo pipefail\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run python -m scripts.tok_train --max_chars=\"${NANOCHAT_TOKENIZER_MAX_CHARS}\"\n",
        "uv run python -m scripts.tok_eval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dbee81b",
      "metadata": {},
      "source": [
        "## Step 8: Fetch the evaluation bundle (CORE metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef449f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env NANOCHAT_EVAL_BUNDLE_URL --env NANOCHAT_EVAL_BUNDLE_DIR\n",
        "\n",
        "set -euo pipefail\n",
        "if [ -f \"$NANOCHAT_EVAL_BUNDLE_DIR/metadata.json\" ]; then\n",
        "  echo \"Eval bundle already present at $NANOCHAT_EVAL_BUNDLE_DIR\"\n",
        "  exit 0\n",
        "fi\n",
        "tmp_dir=$(mktemp -d)\n",
        "cleanup() { rm -rf \"$tmp_dir\"; }\n",
        "trap cleanup EXIT\n",
        "curl -L -o \"$tmp_dir/eval_bundle.zip\" \"$NANOCHAT_EVAL_BUNDLE_URL\"\n",
        "unzip -q \"$tmp_dir/eval_bundle.zip\" -d \"$tmp_dir\"\n",
        "mkdir -p \"$(dirname \"$NANOCHAT_EVAL_BUNDLE_DIR\")\"\n",
        "rm -rf \"$NANOCHAT_EVAL_BUNDLE_DIR\"\n",
        "mv \"$tmp_dir/eval_bundle\" \"$NANOCHAT_EVAL_BUNDLE_DIR\"\n",
        "echo \"Eval bundle ready at $NANOCHAT_EVAL_BUNDLE_DIR\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2959359a",
      "metadata": {},
      "source": [
        "## Step 9 (Optional): Log in to Weights & Biases\n",
        "Set `CONFIG['wandb_login'] = True` in the configuration cell if you want live dashboards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07473c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "flag = os.environ.get('NANOCHAT_WANDB_LOGIN', 'False').lower() in {'1', 'true', 'yes'}\n",
        "if flag:\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "else:\n",
        "    print('Skipping wandb login. Enable CONFIG[\"wandb_login\"] to opt in.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5b5c25",
      "metadata": {},
      "source": [
        "## Step 10: Pretrain the base model\n",
        "This mirrors `scripts/base_train.py`. Expect several hours depending on the preset and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f69429",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_MODEL_DEPTH --env NANOCHAT_DEVICE_BATCH_SIZE --env NANOCHAT_MAX_SEQ_LEN --env NANOCHAT_NUM_ITERATIONS --env NANOCHAT_RUN_NAME --env NANOCHAT_NPROC\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/base_train.py       --depth=\"${NANOCHAT_MODEL_DEPTH}\"       --device_batch_size=\"${NANOCHAT_DEVICE_BATCH_SIZE}\"       --max_seq_len=\"${NANOCHAT_MAX_SEQ_LEN}\"       --num_iterations=\"${NANOCHAT_NUM_ITERATIONS}\"       --run=\"${NANOCHAT_RUN_NAME}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3569b2a",
      "metadata": {},
      "source": [
        "## Step 11: Evaluate the base model (loss + CORE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc6bb22",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_DEVICE_BATCH_SIZE --env NANOCHAT_NPROC\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/base_loss.py       --device_batch_size=\"${NANOCHAT_DEVICE_BATCH_SIZE}\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/base_eval.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e813ed3",
      "metadata": {},
      "source": [
        "## Step 12: Midtrain on conversational data + tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9e7629",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_DEVICE_BATCH_SIZE --env NANOCHAT_NPROC --env NANOCHAT_RUN_NAME\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/mid_train.py       --device_batch_size=\"${NANOCHAT_DEVICE_BATCH_SIZE}\"       --run=\"${NANOCHAT_RUN_NAME}_mid\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cfac493",
      "metadata": {},
      "source": [
        "## Step 13: Evaluate the midtrained chat model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a64cb20",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_NPROC\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/chat_eval.py -i mid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668f3920",
      "metadata": {},
      "source": [
        "## Step 14: Supervised finetuning (SFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e427069",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_DEVICE_BATCH_SIZE --env NANOCHAT_NPROC --env NANOCHAT_RUN_NAME\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/chat_sft.py       --device_batch_size=\"${NANOCHAT_DEVICE_BATCH_SIZE}\"       --run=\"${NANOCHAT_RUN_NAME}_sft\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0561278f",
      "metadata": {},
      "source": [
        "## Step 15: Evaluate the SFT checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86bdb91a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_NPROC\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/chat_eval.py -i sft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcbcdde",
      "metadata": {},
      "source": [
        "## Step 16 (Optional): Reinforcement learning on GSM8K\n",
        "RLHF is optional and slow but can improve math accuracy. Uncomment and run the command below when you have spare budget.\n",
        "\n",
        "```bash\n",
        "%%bash --env PROJECT_ROOT --env NANOCHAT_DEVICE_BATCH_SIZE --env NANOCHAT_NPROC\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/chat_rl.py       --device_batch_size=\"${NANOCHAT_DEVICE_BATCH_SIZE}\"       --run=\"${NANOCHAT_RUN_NAME}_rl\"\n",
        "uv run torchrun --standalone --nproc_per_node=\"${NANOCHAT_NPROC}\" scripts/chat_eval.py -i rl -a GSM8K\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3763e19f",
      "metadata": {},
      "source": [
        "## Step 17: Chat with the model (single prompt demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974cc00f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT --env NANOCHAT_CHAT_PROMPT\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run python -m scripts.chat_cli -i sft -p \"${NANOCHAT_CHAT_PROMPT}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6bdd60",
      "metadata": {},
      "source": [
        "## Step 18: Sync checkpoints to Google Drive (if mounted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc19a8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = os.environ.get('PROJECT_ROOT', '/content/nanochat')\n",
        "source_dir = Path(project_root) / 'checkpoints'\n",
        "drive_dir = os.environ.get('NANOCHAT_DRIVE_CHECKPOINTS_DIR')\n",
        "\n",
        "if not source_dir.exists():\n",
        "    print(f'No checkpoints found at {source_dir}. Run training first.')\n",
        "elif drive_dir and os.path.isdir(drive_dir):\n",
        "    target = Path(drive_dir) / 'latest'\n",
        "    target.mkdir(parents=True, exist_ok=True)\n",
        "    print(f'Syncing checkpoints to {target}...')\n",
        "    shutil.copytree(source_dir, target, dirs_exist_ok=True)\n",
        "    print('Sync complete.')\n",
        "else:\n",
        "    print('Drive not mounted or destination unavailable; skipping sync.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f747646",
      "metadata": {},
      "source": [
        "## Step 19: Export the SFT model to Hugging Face format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05ba13e",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash --env PROJECT_ROOT\n",
        "set -euo pipefail\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "if [ -f \"$HOME/.cargo/env\" ]; then\n",
        "  source \"$HOME/.cargo/env\"\n",
        "fi\n",
        "cd \"$PROJECT_ROOT\"\n",
        "uv run python scripts/export_to_huggingface.py --source sft -o exports/hf_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "029fc2f4",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Review the generated `HYPERBOLIC_DEPLOYMENT_SUMMARY.md` and report artifacts for run metadata.\n",
        "- Sweep different depths or batch sizes by editing the configuration cell and re-running the relevant stages.\n",
        "- Share the exported `exports/hf_model` directory or upload it directly to the Hugging Face Hub.\n",
        "- Tweak datasets or add new mid/SFT mixtures to go beyond the baseline speedrun recipe."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}